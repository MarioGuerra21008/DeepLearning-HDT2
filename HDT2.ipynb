{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÑ Hoja de Trabajo 2\n",
    "Integrantes\n",
    "- Diego Alexander Hern√°ndez Silvestre - 21270\n",
    "- Mario Antonio Guerra Morales - 21008\n",
    "- Linda In√©s Jim√©nez Vides 21169\n",
    "\n",
    "## üíª Ejercicio 1 \n",
    "### Preparaci√≥n del conjunto de datos üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de entrenamiento: 105 muestras\n",
      "Conjunto de validaci√≥n: 45 muestras\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el conjunto de datos de Iris\n",
    "iris = load_iris()\n",
    "X = iris.data  # Caracter√≠sticas\n",
    "y = iris.target  # Etiquetas\n",
    "\n",
    "df_iris = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y validaci√≥n\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Mostrar los tama√±os de los conjuntos resultantes\n",
    "print(f\"Conjunto de entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"Conjunto de validaci√≥n: {X_test.shape[0]} muestras\")\n",
    "df_iris.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleFeedforwardNN(\n",
      "  (fc1): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Red neuronal feedforward\n",
    "class SimpleFeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(SimpleFeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)  # Capa de entrada\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # Capa oculta\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)  # Capa de salida\n",
    "        \n",
    "        self.relu = nn.ReLU()  # Funci√≥n de activaci√≥n ReLU\n",
    "        self.softmax = nn.Softmax(dim=1)  # Funci√≥n de activaci√≥n Softmax para la capa de salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pasar los datos a trav√©s de la red\n",
    "        x = self.relu(self.fc1(x))  \n",
    "        x = self.relu(self.fc2(x))  \n",
    "        x = self.softmax(self.fc3(x)) \n",
    "        return x\n",
    "\n",
    "# Par√°metros de la red\n",
    "input_size = 4  # N√∫mero de caracter√≠sticas de entrada \n",
    "hidden_size1 = 10  # N√∫mero de neuronas en la primera capa oculta\n",
    "hidden_size2 = 8  # N√∫mero de neuronas en la segunda capa oculta\n",
    "output_size = 3  # N√∫mero de clases de salida \n",
    "\n",
    "# Instancia de la red neuronal\n",
    "model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Mostrar la estructura de la red\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìâ Funciones de perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Verificar si CUDA est√° disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convertir a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Crear DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Function: CrossEntropy\n",
      "Final Training Loss: 0.5754\n",
      "Final Test Loss: 0.5553\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Loss Function: MSE\n",
      "Final Training Loss: 0.0102\n",
      "Final Test Loss: 0.0063\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Loss Function: NLL\n",
      "Final Training Loss: 0.5840\n",
      "Final Test Loss: 0.5670\n",
      "Accuracy: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def loss_functions():\n",
    "    # Definir las funciones de p√©rdida a comparar\n",
    "    loss_functions = [\n",
    "        ('CrossEntropy', nn.CrossEntropyLoss()),\n",
    "        ('MSE', nn.MSELoss()),\n",
    "        ('NLL', nn.NLLLoss())\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Iterar sobre cada funci√≥n de p√©rdida\n",
    "    for loss_name, criterion in loss_functions:\n",
    "        # Inicializar el modelo y el optimizador\n",
    "        model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        \n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        # Entrenamiento durante 100 √©pocas\n",
    "        for epoch in range(100):\n",
    "            model.train()  # Poner el modelo en modo entrenamiento\n",
    "            outputs = model(X_train_tensor.to(device))  # Hacer una pasada hacia adelante\n",
    "            \n",
    "            # Calcular la p√©rdida seg√∫n la funci√≥n de p√©rdida\n",
    "            if loss_name == 'CrossEntropy':\n",
    "                loss = criterion(outputs, y_train_tensor.to(device))\n",
    "            elif loss_name == 'NLL':\n",
    "                log_probs = nn.functional.log_softmax(outputs, dim=1)\n",
    "                loss = criterion(log_probs, y_train_tensor.to(device))\n",
    "            else:\n",
    "                # 'MSE' usa etiquetas one-hot\n",
    "                loss = criterion(outputs, nn.functional.one_hot(y_train_tensor, num_classes=output_size).float().to(device))\n",
    "            \n",
    "            # Actualizar los gradientes\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Evaluar el modelo\n",
    "            model.eval()  # Poner el modelo en modo evaluaci√≥n\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(X_test_tensor.to(device))  # Hacer una pasada hacia adelante en el conjunto de prueba\n",
    "                \n",
    "                # Calcular la p√©rdida en el conjunto de prueba\n",
    "                if loss_name == 'CrossEntropy':\n",
    "                    test_loss = criterion(test_outputs, y_test_tensor.to(device))\n",
    "                elif loss_name == 'NLL':\n",
    "                    log_probs = nn.functional.log_softmax(test_outputs, dim=1)\n",
    "                    test_loss = criterion(log_probs, y_test_tensor.to(device))\n",
    "                else:\n",
    "                    # 'MSE' usa etiquetas one-hot\n",
    "                    test_loss = criterion(test_outputs, nn.functional.one_hot(y_test_tensor, num_classes=output_size).float().to(device))\n",
    "                \n",
    "                test_losses.append(test_loss.item())\n",
    "        \n",
    "        # Calcular la precisi√≥n\n",
    "        model.eval()  # Poner el modelo en modo evaluaci√≥n\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test_tensor.to(device))\n",
    "            _, predicted = torch.max(test_outputs.data, 1)  # Obtener las predicciones de clase\n",
    "            accuracy = (predicted == y_test_tensor.to(device)).sum().item() / y_test_tensor.size(0)  # Calcular la precisi√≥n\n",
    "        \n",
    "        # Almacenar los resultados\n",
    "        results.append((loss_name, train_losses[-1], test_losses[-1], accuracy))\n",
    "    \n",
    "    # Imprimir los resultados\n",
    "    for loss_name, train_loss, test_loss, accuracy in results:\n",
    "        print(f\"Loss Function: {loss_name}\")\n",
    "        print(f\"Final Training Loss: {train_loss:.4f}\")\n",
    "        print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print()\n",
    "\n",
    "loss_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä T√©cnicas de Regularizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization: None\n",
      "Test Loss: 0.5584\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Regularization: L1\n",
      "Test Loss: 0.8194\n",
      "Accuracy: 0.7111\n",
      "\n",
      "Regularization: L2\n",
      "Test Loss: 0.6095\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Regularization: Dropout\n",
      "Test Loss: 0.5988\n",
      "Accuracy: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def regularization():\n",
    "    # Tipos de regularizaci√≥n a evaluar\n",
    "    reg_types = ['None', 'L1', 'L2', 'Dropout']\n",
    "    results = []\n",
    "    \n",
    "    # Iterar sobre cada tipo de regularizaci√≥n\n",
    "    for reg_type in reg_types:\n",
    "        # Inicializar el modelo para cada tipo de regularizaci√≥n\n",
    "        model = SimpleFeedforwardNN(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "        \n",
    "        # Aplicar la regularizaci√≥n seg√∫n el tipo\n",
    "        if reg_type == 'Dropout':\n",
    "            model.fc1 = nn.Sequential(\n",
    "                model.fc1,\n",
    "                nn.Dropout(0.5)  # Aplicar Dropout con probabilidad del 50%\n",
    "            )\n",
    "            model.fc2 = nn.Sequential(\n",
    "                model.fc2,\n",
    "                nn.Dropout(0.5)  # Aplicar Dropout con probabilidad del 50%\n",
    "            )\n",
    "        \n",
    "        # Definir la funci√≥n de p√©rdida y el optimizador\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        \n",
    "        # Entrenamiento durante 100 √©pocas\n",
    "        for epoch in range(100):\n",
    "            model.train()  # Poner el modelo en modo entrenamiento\n",
    "            outputs = model(X_train_tensor.to(device))  # Hacer una pasada hacia adelante\n",
    "            \n",
    "            # Calcular la p√©rdida\n",
    "            loss = criterion(outputs, y_train_tensor.to(device))\n",
    "            \n",
    "            # Aplicar regularizaci√≥n L1 o L2 si corresponde\n",
    "            if reg_type == 'L1':\n",
    "                l1_lambda = 0.01  # Hiperpar√°metro de regularizaci√≥n L1\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())  # Calcular la norma L1\n",
    "                loss = loss + l1_lambda * l1_norm  # Agregar la regularizaci√≥n L1 a la p√©rdida\n",
    "            elif reg_type == 'L2':\n",
    "                l2_lambda = 0.01  # Hiperpar√°metro de regularizaci√≥n L2\n",
    "                l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())  # Calcular la norma L2\n",
    "                loss = loss + l2_lambda * l2_norm  # Agregar la regularizaci√≥n L2 a la p√©rdida\n",
    "            \n",
    "            # Actualizar los gradientes\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluar el modelo\n",
    "        model.eval()  # Poner el modelo en modo evaluaci√≥n\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test_tensor.to(device))  # Hacer una pasada hacia adelante en el conjunto de prueba\n",
    "            _, predicted = torch.max(test_outputs.data, 1)  # Obtener las predicciones de clase\n",
    "            accuracy = (predicted == y_test_tensor.to(device)).sum().item() / y_test_tensor.size(0)  # Calcular la precisi√≥n\n",
    "            test_loss = criterion(test_outputs, y_test_tensor.to(device))  # Calcular la p√©rdida en el conjunto de prueba\n",
    "        \n",
    "        # Almacenar los resultados\n",
    "        results.append((reg_type, test_loss.item(), accuracy))\n",
    "    \n",
    "    # Imprimir los resultados para cada tipo de regularizaci√≥n\n",
    "    for reg_type, test_loss, accuracy in results:\n",
    "        print(f\"Regularization: {reg_type}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print()\n",
    "\n",
    "regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Algoritmos de Optimizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Experimentaci√≥n y An√°lisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó£Ô∏è Discusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Ejercicio 2 - Attention is all you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **¬øCu√°l es la principal innovaci√≥n de la arquitectura Transformer?**\n",
    "2. **¬øC√≥mo funciona el mecanismo de atenci√≥n del scaled dot-product?**\n",
    "3. **¬øPor qu√© se utiliza la atenci√≥n de m√∫ltiples cabezales en Transformer?**\n",
    "4. **¬øC√≥mo se incorporan los positional encodings en el modelo Transformer?**\n",
    "5. **¬øCu√°les son algunas aplicaciones de la arquitectura Transformer m√°s all√° de la machine translation?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
